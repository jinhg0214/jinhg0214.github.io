---
title: "DeepSeek-R1 로컬 환경에서 구동하기"
date: 2025-01-31 16:00:00 +0900
categories: [IT, Ai]  # 최대 2개 가능
tags: [chatgpt, deepseek, chatbox, local, ollama, docker]     # 태그는 항상 소문자로 작성할 것
toc: true
comment: false
published: true
image:
    path: "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRuWcHy3g-3ZEXm5LYgrnllnli_c0qqmF4lgw&s"
    alt: 
---

1월 말 전세계를 충격에 빠트릴만한 뉴스가 나왔다

바로 중국에서 ChatGPT를 따라잡을만한 AI를 개발했다는 것

미국의 제재로 사용 금지된 엔비디아 고사양 그래픽카드 대신, AMD를 개조해서 개발했다는 소식에

엔비디아 주가가 급락하기도 했다

## 1. 개요
---

![deepseek](https://bsmedia.business-standard.com/_media/bs/img/article/2025-01/27/full/1737959259-7169.png?im=FeatureCrop,size=(826,465))

DeepSeek(深度求索, 심도구색)는 중국 항주에 본사를 둔 인공지능 기업   

'환팡퀀트(幻方量化)' 소속 인공지능 연구 기업의 이름이자, 같은 회사에서 개발한 언어 모델이다.   

오픈소스로 공개되어있어, 모든 사용자들이 코드에 접근 가능하고,

상대적으로 경량화 된 모델을 제공하여 속도가 빠르다   

심지어 [라즈베리파이에서 구동 하는 영상](https://www.youtube.com/watch?v=o1sN1lB76EA&ab_channel=JeffGeerling)도 있다

### ChatGPT와 차이점
- DeepSeek의 개발 비용은 약 80억원으로, GPT-4와비교하면 상당히 저렴하다
- 단계적으로 응답을 생성하는 과정, 추론 과정을 직접 볼 수 있다. 
- 혼합 전문가(MoE, Mixture-of-Experts) 
	- 전체 매개변수 중 일부만 활성화 하는 방식으로 연산 자원을 절약함
	- 비유하자면, ChatGPT는 척척박사님 한명이 모든 질문에 답변해주는것이라면,   
	  DeepSeek은 여러 전문가들이 있는데 자신의 분야의 질문이 나오면 그 전문가만 나와서 답변해주는 것 
- 다중 헤드 잠재 어텐션(MLA, Multi-head Latent Attention)
		- 낮은 메모리 사용량을 요구해 처리 속도를 높임
- 오픈소스이므로 모든 코드가 공개되어있고, 무료로 사용 가능하다
- 로컬 환경에서도 구동 가능하다

## 2. 사용법
---

![Image](https://github.com/user-attachments/assets/7933d07b-c521-40eb-9c81-22ee94feafb0)

가장 간단한 방법으로는 

ChatGPT와 같이 https://www.deepseek.com/ 에 접속하여 가입 후 사용하는 것이다

그러나 온라인에서 사용하는 것은 몇가지 문제점이 있다

### 온라인 사용의 문제점

1. 인터넷 연결 필수 : 질문을 전송하고 수신하는 과정에서 인터넷이 필요함

2. 검열 문제 : 각종 주제에 대한 검열이 존재할 수 있음

3. 개인 정보 문제 : 자신의 개인 정보가 "서버"에 저장됨

특히 DeepSeek은 중국에서 개발한 인공지능이기 때문에 개인정보 유출에 민감할 수 밖에 없다

해외 언론에서도 DeepSeek은 타 AI에 비해 너무 많은 개인정보를 수집한다는 기사를 내놓고 있다

입력한 모든 데이터, 키 입력 패턴, IP 및 위치 데이터부터

텍스트, 오디오, 채팅 기록을 포함한 모든 사용자 입력, 폰 모델, 운영체제, IP주소 등 엄청나게 많은 데이터를 수집하고 있다

이 데이터들이 중국 법의 적용을 받기 때문에, 어떻게 사용될지는 알 수 없다

이를 우려하는 일부 사용자들은 가벼운 모델의 특성을 활용해, 

로컬 환경에서 오프라인으로 deepseek을 활용하고 있다

우리도 로컬 환경에서 DeepSeek을 설치하고 사용하는 방법을 알아보겠다

### 사양
- OS : Windows 10
- CPU : AMD 5800x
- GPU : NVIDIA RTX 3080 (10GB VRAM)
- RAM : 48GB

간단하게 설명하면, 도커화해서 거기다가 ollama, open-webui, deepseek-r1:14b 넣고 쓸거임

## 3. DeepSeek-R1 로컬 설치하기
---

먼저 대규모 언어 모델을 돌리기 위한 도구를 설치해야한다

### 1. Ollama 설치

![Image](https://github.com/user-attachments/assets/aa50b1f7-6de1-4b09-a1fb-ee8961b92b50)

Ollama는 오픈소스 대형 언어모델(LLM : Large Language Model)을 로컬 PC에서 쉽게 실행할 수 있도록 도와주는 프로그램이다

Llama, DeepSeek 등 다양한 언어 모델을 지원한다

[Ollama 공식 홈페이지](https://ollama.com/)의 다운로드 버튼, 혹은 [Ollama 의 깃허브](https://github.com/ollama/ollama?tab=readme-ov-file))에서 

자신의 운영체제에 맞는 최신버전을 받아서 설치한다

![Image](https://github.com/user-attachments/assets/da745f7c-ce3a-41d1-adc1-88d09fe0cf73)

설치가 완료됬다면 윈도우 프롬프트에서 `ollama` 입력시 다음과 같이 출력되면 정상적으로 설치 성공!

### 2. DeepSeek-r1 모델 다운

Ollama를 설치했다면, CMD에서 바로 쉽게 설치할 수 있다

[Ollama DeepSeek-r1](https://ollama.com/library/deepseek-r1)에서 자신의 사양에 맞는 버전을 확인 후,

`ollama run deepseek-r1:{파라미터 크기}`와 같이 입력하면 자동으로 다운로드 된다

한국어로 사용하고 싶다면, 14b, 32b를 추천

나는 사양 이슈로 14b를 선택하여 `ollama run deepseek-r1:14b`로 설치했다

- 모델 설치 기본 경로는 `C:\Users\사용자명\.ollama\models` 폴더에 설치 되므로, 경로를 바꾸고 싶다면 윈도우 환경 변수를 수정해야한다

- 먼저 실행중인 Ollama를 끈다. 작업관리자에서 확인할 것

- `시스템 환경 변수 편집 > 시스템 속성 > 고급 > 환경 변수 > 시스템 변수(S) > 새로 만들기`에

- 변수 이름은 `OLLAMA_MODELS`, 변수 값은 자신이 원하는 경로로 설정 후 저장

![Image](https://github.com/user-attachments/assets/2b160538-17a7-460e-be36-c5aeb08b46fa)

UI 없이 CMD창에서도 충분히 동작하지만, CLI 방식이 불편하다면 Chatbox 혹은 open-webui를 추가로 설치해야한다

여기서는 조금 더 나아가서, 도커를 사용해서 컨테이너를 통해 Ollama를 관리해보겠다

### 3. Docker Desktop 설치

도커를 사용하는 이유는 몇가지가 있다

1. 환경 격리 : 호스트 OS에 영향을 주지 않고, LLM을 격리해서 사용할 수 있음

2. 빠른 배포 및 확장 : 가상머신보다 훨씬 빠르고, 오버헤드가 적음

3. 백업 및 복구가 용이함 : 쉽게 백업본을 생성하고, 복구할 수 있음

이러한 이유로 도커화를 선택했다

![Image](https://github.com/user-attachments/assets/1307f57b-64d7-4f17-9816-d0cedff179c1)

[도커 공식 홈페이지](https://www.docker.com/products/docker-desktop/)에서 데스크탑을 설치한다

이후 설치하고자하는 경로로 이동하여 

```yml
services:  
  ollama:  
    image: ollama/ollama:latest  
    container_name: ollama  
    volumes:  
      - ollama:/root/.ollama  
    networks:  
      - llm-network  
    deploy:  
      resources:  
        reservations:  
          devices:  
            - driver: nvidia  
              count: all  
              capabilities: [gpu]

  open-webui:  
    image: ghcr.io/open-webui/open-webui:main  
    container_name: open-webui  
    ports:  
      - "18080:8080"  
    environment:  
      - OLLAMA_BASE_URL=http://ollama:11434  
    volumes:  
      - open-webui:/app/backend/data  
    restart: always  
    networks:  
      - llm-network

volumes:  
  ollama:  
    driver: local  
  open-webui:  
    driver: local

networks:  
  llm-network:  
    driver: bridge
```

위의 텍스트 파일을 `docker-compose.yml`로 저장한뒤, 설치하려는 폴더에 해당 파일을 넣는다

(Ollama와 Open WebUI를 Docker 컨테이너로 실행하는 설정임)

![Image](https://github.com/user-attachments/assets/443694cc-3b11-4180-a1bd-ac81ed15199a)

해당 폴더에서 CMD 를 관리자 권한으로 실행한 뒤 `docker_compose up`을 실행하면 컨테이너가 생성된다

![Image](https://github.com/user-attachments/assets/dd7503fd-18f5-453b-8a8e-16f831c97fc5)

설치 후, 도커 데스크톱을 실행하여 해당 컨테이너를 실행한다

리스트 오른쪽 위의 재생 버튼을 눌러 실행시키고, 초록불이 들어왔다면 정상실행 성공!

이후 18080:8080에 접속하면 open-webui에 접속할 수 있다

(해당 포트에 접속하면 가입하라고 뜨는데 어짜피 로컬에만 저장되므로 아무거나 편한걸로 아이디 생성하면 됨)

### 4. 컨테이너에 DeepSeek 설치

위에서 받은 DeepSeek은, Ollama에서 CLI로 사용하는 환경이였기 때문에 다시 받아야한다

![Image](https://github.com/user-attachments/assets/75340bea-9c44-4e2f-ad2a-574383868945)

WebUI 좌측 하단의 프로필 > 설정 > 관리자 설정 > 모델에 들어간다

![Image](https://github.com/user-attachments/assets/fd6ea7b5-8727-4de4-b1d0-d0621468b257)

우측 상단의 Manage Models 를 눌러 모델 가져오기 부분에 설치하고자 하는 모델을 입력한다 

![Image](https://github.com/user-attachments/assets/19792951-1c67-4006-88e2-57931f512438)

이후 위 사진처럼 일반적인 챗봇처럼 사용할 수 있다

성능은 14b라 그런지 헛소리를 자주하긴 하는데, 검열 없이, 개인정보 유출 걱정 없이 사용할 수 있다는 점에서 

매우 유용하게 사용할 수 있을 것 같다 

---

하나씩 배워가면서 순서가 엉망진창인 부분이 좀 있다

모델을 두번이나 다운로드하고, ollama를 메인 OS에 깔았다가, 도커에 깔았다가 하는데

뭔가 잘못된걸 느낀다면 저보다 도커를 잘 아는 분일테니 화이팅